{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tables\n",
      "  Downloading tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables) (1.18.5)\n",
      "Collecting numexpr>=2.6.2\n",
      "  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 69.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numexpr, tables\n",
      "Successfully installed numexpr-2.7.1 tables-3.6.1\n"
     ]
    }
   ],
   "source": [
    "#if youre running on colab, run this line first to properly load the h5 files\n",
    "# !pip install tables --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers.comet import CometLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.5'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maybe downgrade?\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "Successfully installed numpy-1.18.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "## Point Cloud MNIST with DeepSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below you have a custom dataloader for the point-cloud MNIST dataset,\n",
    "\n",
    "the training and validation datasets are linked from the course website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.df = pd.read_hdf(path)\n",
    "        \n",
    "        self.label = torch.LongTensor(self.df.label)\n",
    "        \n",
    "        self.n_points = self.df.n_points\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "       \n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "    \n",
    "        return torch.FloatTensor(self.df.iloc[idx].xy), self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset('./storage/data_for_pointcloud_MNIST/training_ds.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = CustomDataset('training_ds.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the data is exactly like the MNIST dataset, except that instead of a 28x28 image,\n",
    "#### you get a (N x 2) array of points (different number of points for each item in the dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "xy = ds[445][0]\n",
    "\n",
    "ax.scatter( xy[:,0],xy[:,1] )\n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the dataset object has a n_points variable that tells us how many points in each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ds.n_points,np.linspace(19.5,260.5,242))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One way to deal with this variable size is to use a custom Batch Sampler\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "This object will tell our dataloader which item indices to request for the batches - \n",
    "and we can \"rig\" it to return batches where all the items have the same N, and therefore we can stack them without a custom colate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, points_per_entry, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.N_per_entry = points_per_entry\n",
    "        self.batches = {}\n",
    "        # hacky\n",
    "        self.n_batches = 246\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        \n",
    "        self.entries_with_N = {}\n",
    "        running_idx = -1\n",
    "\n",
    "        for N in set(self.N_per_entry):\n",
    "            \n",
    "            self.entries_with_N[N] = np.where(self.N_per_entry == N)[0]\n",
    "\n",
    "            how_many = len(self.entries_with_N[N])\n",
    "            n_batches = np.amax([ how_many / self.batch_size, 1])\n",
    "\n",
    "            self.entries_with_N[N] = np.array_split(np.random.permutation(self.entries_with_N[N]),\n",
    "                                                           n_batches)\n",
    "            for batch in self.entries_with_N[N]:\n",
    "                running_idx += 1\n",
    "                self.batches[running_idx] = batch\n",
    "        self.n_batches = running_idx + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        self.generate_batches()\n",
    "        \n",
    "        batch_order = np.random.permutation(np.arange(self.n_batches))\n",
    "        for i in batch_order:\n",
    "            yield self.batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batch_sampler = CustomBatchSampler(ds.n_points, batch_size)\n",
    "data_loader = DataLoader(ds, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in data_loader:\n",
    "    x,y = sample\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a DeepSet model\n",
    "\n",
    "you only have three components - a fully connected network that creates the node embedding, a sum operation, and a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([345, 5]), torch.Size([345, 76, 5]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the linear layer operates on the last dimension:\n",
    "\n",
    "linear_layer = nn.Linear(10,5)\n",
    "\n",
    "x1,x2 = linear_layer(  torch.rand((345,10)) ), linear_layer(  torch.rand((345,76,10)) )\n",
    "x1.shape, x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 5])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x2,dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the the mean operation you need to specify the dimension:\n",
    "\n",
    "x = torch.rand((42,15,10))\n",
    "\n",
    "torch.mean(x,dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the model, train, submit when you reach above 75% accuracy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_data = './storage/data_for_pointcloud_MNIST/training_ds.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_validation_data = './storage/data_for_pointcloud_MNIST/valid_ds.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNC = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageClassifier(nn.Module):\n",
    "class DeepSet(pl.LightningModule):\n",
    "    def __init__(self):        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hidsize = 2048\n",
    "        \n",
    "        \n",
    "        #MOD\n",
    "        self.embedding = nn.Linear(2,self.hidsize).requires_grad_(False)\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        \n",
    "            \n",
    "#             nn.Linear(2,hidsize)\n",
    "            nn.ReLU(True),\n",
    "            # try batchnorm, dropout\n",
    "            #nn.Dropout(),\n",
    "#             nn.Linear(hidsize, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             # MOD\n",
    "#             nn.Dropout(),\n",
    "            #num_classes = 10\n",
    "            nn.Linear(self.hidsize, NUM_CLASSES),\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "#         self.hidsize = 2048\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = self.embedding(batch)\n",
    "        #MOD..also, requires_grad?\n",
    "        out = torch.mean(out,dim=1).clone().detach().requires_grad_(False)\n",
    "        \n",
    "#         out = self.features(x)\n",
    "        \n",
    "#         # what does this accomplish \n",
    "#         out = self.avgpool(out)\n",
    "#         out = torch.flatten(out,1)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        ##### pass through the rest of your model\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset_train = CustomDataset(path_to_training_data)\n",
    "        batch_sampler = CustomBatchSampler(dataset_train.n_points, batch_size=50)\n",
    "#         self.Ns = batch_sampler.entries\n",
    "        return DataLoader(dataset_train,batch_sampler=batch_sampler,num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset_val = CustomDataset(path_to_validation_data)\n",
    "        batch_sampler_val = CustomBatchSampler(dataset_val.n_points,batch_size=50)\n",
    "        return DataLoader(dataset_val,batch_sampler=batch_sampler_val,num_workers=4)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        print('batch_ix',batch_idx)\n",
    "        print('batch',batch)\n",
    "        minib,target = batch\n",
    "        yhat = self(minib)\n",
    "        print('yhat',yhat)\n",
    "        print('target',target)\n",
    "#         out = self.forward(minib)\n",
    "        loss = LOSS_FUNC(yhat,target)\n",
    "        print('loss',loss)\n",
    "        \n",
    "    \n",
    "        # try this\n",
    "        pred = yhat.argmax(dim=1, keepdim=True) #get ix of max log-proba\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        accuracy = correct/len(minib)\n",
    "        \n",
    "        # add logging\n",
    "        logs = {'train_loss': loss,'train_accuracy':accuracy}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "#         return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        minib, target = batch\n",
    "#         out = self.forward(minib)\n",
    "        yhat = self(minib)\n",
    "        loss = LOSS_FUNC(yhat,target)\n",
    "        \n",
    "        pred = out.argmax(dim=1, keepdim=True) #get ix of max log-proba\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        accuracy = correct/len(minib)\n",
    "        \n",
    "#         logs = {'train_loss': loss,'val_accuracy':accuracy}\n",
    "#         return {'val_loss': loss, 'log': logs}\n",
    "        \n",
    "        return OrderedDict({'val_step_loss':loss, 'val_step_acc':accuracy})\n",
    "        \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #TODO imbalanced given each batch is of a different size...\n",
    "        avg_loss = torch.stack([x['val_step_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_step_acc'] for x in outputs]).mean()\n",
    "        \n",
    "        \n",
    "        tqdm_dict = {'val_acc': avg_acc.item()}\n",
    "\n",
    "        # show val_acc in progress bar but only log val_loss\n",
    "        results = {\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': {'val_acc': avg_acc.item(),\n",
    "                    'val_loss': avg_loss.item()\n",
    "                   }\n",
    "        }\n",
    "        return results\n",
    "#         logs = {'val_loss':avg_loss}\n",
    "#         return {'avg_val_loss':avg_loss, 'log':tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 87, 2])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "# class MyPrintingCallback(Callback):\n",
    "\n",
    "#     def on_init_start(self, trainer):\n",
    "#         print('Starting to init trainer!')\n",
    "\n",
    "#     def on_init_end(self, trainer):\n",
    "#         print('Trainer is init now')\n",
    "\n",
    "#     def on_train_end(self, trainer, pl_module):\n",
    "#         print('do something when training ends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(callbacks=[MyPrintingCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'deepsettest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/ren-e1011/dl1010-a3/5c63ddf0320c4217acc6334af5931080\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : deepsettest\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: old comet version (3.1.9) detected. current: 3.1.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ren-e1011/dl1010-a3/e649bb912f92426bb6bf372a670c67b8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comet_logger = CometLogger(\n",
    "    api_key='n0QCcEJ7YYeDUkff49kqLEdeJ',\n",
    "    workspace=\"ren-e1011\",\n",
    "    project_name=\"dl1010-a3\", \n",
    "    experiment_name=experiment  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: True, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(fast_dev_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type       | Params\n",
      "----------------------------------------\n",
      "0 | embedding    | Linear     | 6 K   \n",
      "1 | classifier   | Sequential | 20 K  \n",
      "2 | classifier.0 | ReLU       | 0     \n",
      "3 | classifier.1 | Linear     | 20 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd53aab5cf44608be0758d9f5325efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_ix 0\n",
      "batch [tensor([[[ 0.0000,  0.1429],\n",
      "         [ 0.0000,  0.6429],\n",
      "         [ 0.0000, -0.5714],\n",
      "         [ 0.0000, -0.6429],\n",
      "         [-0.0714, -0.4286],\n",
      "         [ 0.0000,  0.0714],\n",
      "         [ 0.0714,  0.3571],\n",
      "         [-0.1429, -0.3571],\n",
      "         [ 0.0714,  0.2857],\n",
      "         [ 0.0000, -0.2143],\n",
      "         [-0.0714,  0.0714],\n",
      "         [ 0.0000,  0.7143],\n",
      "         [ 0.0714,  0.2143],\n",
      "         [ 0.0000,  0.4286],\n",
      "         [-0.0714, -0.5000],\n",
      "         [-0.0714, -0.5714],\n",
      "         [-0.0714, -0.1429],\n",
      "         [ 0.0000,  0.2857],\n",
      "         [ 0.0000,  0.3571],\n",
      "         [ 0.0000,  0.2143],\n",
      "         [-0.1429, -0.2857],\n",
      "         [ 0.0000, -0.0714],\n",
      "         [-0.0714,  0.0000],\n",
      "         [-0.1429, -0.4286],\n",
      "         [ 0.0000, -0.1429],\n",
      "         [ 0.0000,  0.5714],\n",
      "         [-0.0714, -0.2857],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [-0.0714, -0.0714],\n",
      "         [ 0.0000,  0.5000],\n",
      "         [-0.1429, -0.5000],\n",
      "         [-0.0714, -0.3571],\n",
      "         [ 0.0714,  0.1429],\n",
      "         [-0.0714, -0.6429],\n",
      "         [-0.0714, -0.2143]],\n",
      "\n",
      "        [[-0.0714,  0.0714],\n",
      "         [ 0.0000,  0.0714],\n",
      "         [ 0.1429,  0.6429],\n",
      "         [ 0.0000,  0.2143],\n",
      "         [-0.0714, -0.0714],\n",
      "         [-0.2143, -0.6429],\n",
      "         [ 0.0714,  0.4286],\n",
      "         [-0.1429, -0.5714],\n",
      "         [ 0.1429,  0.5714],\n",
      "         [-0.1429, -0.5000],\n",
      "         [-0.1429, -0.2143],\n",
      "         [ 0.0714,  0.5000],\n",
      "         [-0.0714, -0.2143],\n",
      "         [-0.2143, -0.5714],\n",
      "         [ 0.0714,  0.5714],\n",
      "         [ 0.0714,  0.6429],\n",
      "         [ 0.0000,  0.2857],\n",
      "         [-0.0714, -0.2857],\n",
      "         [-0.0714,  0.1429],\n",
      "         [ 0.0000,  0.5000],\n",
      "         [-0.0714, -0.1429],\n",
      "         [-0.0714, -0.3571],\n",
      "         [-0.1429, -0.3571],\n",
      "         [ 0.0000,  0.4286],\n",
      "         [-0.2143, -0.7143],\n",
      "         [ 0.0714,  0.3571],\n",
      "         [ 0.0000, -0.0714],\n",
      "         [ 0.0714,  0.2857],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0714,  0.2143],\n",
      "         [-0.1429, -0.2857],\n",
      "         [-0.1429, -0.4286],\n",
      "         [ 0.0000,  0.1429],\n",
      "         [ 0.0000,  0.3571],\n",
      "         [-0.0714,  0.0000]],\n",
      "\n",
      "        [[-0.0714, -0.0714],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0714],\n",
      "         [ 0.0714,  0.1429],\n",
      "         [ 0.1429,  0.2857],\n",
      "         [ 0.2143,  0.5000],\n",
      "         [-0.2857, -0.4286],\n",
      "         [-0.2857, -0.5000],\n",
      "         [-0.2143, -0.3571],\n",
      "         [-0.2143, -0.2143],\n",
      "         [ 0.0714,  0.2143],\n",
      "         [-0.3571, -0.6429],\n",
      "         [ 0.0714,  0.0714],\n",
      "         [ 0.2143,  0.4286],\n",
      "         [-0.1429, -0.2857],\n",
      "         [ 0.1429,  0.3571],\n",
      "         [-0.0714, -0.1429],\n",
      "         [-0.2857, -0.3571],\n",
      "         [-0.1429, -0.1429],\n",
      "         [ 0.2857,  0.4286],\n",
      "         [ 0.2857,  0.5714],\n",
      "         [ 0.2143,  0.2857],\n",
      "         [-0.2143, -0.2857],\n",
      "         [-0.0714,  0.0000],\n",
      "         [-0.3571, -0.7143],\n",
      "         [-0.3571, -0.5714],\n",
      "         [-0.2143, -0.4286],\n",
      "         [ 0.2143,  0.3571],\n",
      "         [ 0.0000,  0.0714],\n",
      "         [-0.3571, -0.5000],\n",
      "         [-0.1429, -0.2143],\n",
      "         [ 0.2857,  0.6429],\n",
      "         [ 0.2857,  0.5000],\n",
      "         [ 0.1429,  0.2143],\n",
      "         [ 0.1429,  0.1429]]]), tensor([1, 1, 1])]\n",
      "yhat tensor([[ 0.1128,  0.0884,  0.0256, -0.0145,  0.0923,  0.0841, -0.2249, -0.1118,\n",
      "          0.0773, -0.1326],\n",
      "        [ 0.1186,  0.0723,  0.0345, -0.0126,  0.0847,  0.0960, -0.2038, -0.1257,\n",
      "          0.0847, -0.1322],\n",
      "        [ 0.1146,  0.0833,  0.0272, -0.0139,  0.0901,  0.0877, -0.2188, -0.1167,\n",
      "          0.0787, -0.1326]])\n",
      "target tensor([1, 1, 1])\n",
      "loss tensor(2.2276)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-da65be8db77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m     def test(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0m_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0;31m# check if loss or model weights are nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mmodel_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                         \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0;31m# track metrics for callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/hooks.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, trainer, loss, optimizer, optimizer_idx)\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_loader,net):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    total_number = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for x,y in data_loader:\n",
    "        \n",
    "        prediction = net(x).data.numpy()\n",
    "        \n",
    "        prediction = np.argmax(prediction,axis=1)\n",
    "        \n",
    "        correct = len( np.where(prediction==y.data.numpy())[0] )\n",
    "        \n",
    "        total_correct+=correct\n",
    "        total_number+=x.shape[0]\n",
    "        \n",
    "    return total_correct/float(total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CustomDataset('valid_ds.h5')\n",
    "batch_size = 50\n",
    "batch_sampler_test_ds = CustomBatchSampler(test_ds.n_points, batch_size)\n",
    "data_loader_test = DataLoader(test_ds, batch_sampler=batch_sampler_test_ds)\n",
    "\n",
    "compute_accuracy(data_loader_test,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
