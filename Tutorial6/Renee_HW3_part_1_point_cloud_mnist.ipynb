{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if youre running on colab, run this line first to properly load the h5 files\n",
    "# !pip install tables --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers.comet import CometLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maybe downgrade?\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "## Point Cloud MNIST with DeepSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below you have a custom dataloader for the point-cloud MNIST dataset,\n",
    "\n",
    "the training and validation datasets are linked from the course website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.df = pd.read_hdf(path)\n",
    "        \n",
    "        self.label = torch.LongTensor(self.df.label)\n",
    "        \n",
    "        self.n_points = self.df.n_points\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "       \n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "    #mod requires_grad=True\n",
    "        return torch.FloatTensor(self.df.iloc[idx].xy), self.label[idx]\n",
    "#         return torch.tensor(self.df.iloc[idx].xy,dtype=torch.float, requires_grad=True), self.label[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = CustomDataset('./storage/data_for_pointcloud_MNIST/training_ds.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = CustomDataset('training_ds.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset('~/Dropbox/data_for_pointcloud_MNIST/training_ds.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the data is exactly like the MNIST dataset, except that instead of a 28x28 image,\n",
    "#### you get a (N x 2) array of points (different number of points for each item in the dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "xy = ds[445][0]\n",
    "\n",
    "ax.scatter( xy[:,0],xy[:,1] )\n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the dataset object has a n_points variable that tells us how many points in each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ds.n_points,np.linspace(19.5,260.5,242))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One way to deal with this variable size is to use a custom Batch Sampler\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "This object will tell our dataloader which item indices to request for the batches - \n",
    "and we can \"rig\" it to return batches where all the items have the same N, and therefore we can stack them without a custom colate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, points_per_entry, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.N_per_entry = points_per_entry\n",
    "        self.batches = {}\n",
    "        # hacky\n",
    "        self.n_batches = 246\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        \n",
    "        self.entries_with_N = {}\n",
    "        running_idx = -1\n",
    "\n",
    "        for N in set(self.N_per_entry):\n",
    "            \n",
    "            self.entries_with_N[N] = np.where(self.N_per_entry == N)[0]\n",
    "\n",
    "            how_many = len(self.entries_with_N[N])\n",
    "            n_batches = np.amax([ how_many / self.batch_size, 1])\n",
    "\n",
    "            self.entries_with_N[N] = np.array_split(np.random.permutation(self.entries_with_N[N]),\n",
    "                                                           n_batches)\n",
    "            for batch in self.entries_with_N[N]:\n",
    "                running_idx += 1\n",
    "                #MOD\n",
    "#                 self.batches[running_idx] = batch\n",
    "                self.batches[running_idx] = batch\n",
    "                \n",
    "        self.n_batches = running_idx + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        self.generate_batches()\n",
    "        \n",
    "        batch_order = np.random.permutation(np.arange(self.n_batches))\n",
    "        for i in batch_order:\n",
    "            yield self.batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batch_sampler = CustomBatchSampler(ds.n_points, batch_size)\n",
    "data_loader = DataLoader(ds, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in data_loader:\n",
    "    x,y = sample\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a DeepSet model\n",
    "\n",
    "you only have three components - a fully connected network that creates the node embedding, a sum operation, and a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the model, train, submit when you reach above 75% accuracy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_training_data = './storage/data_for_pointcloud_MNIST/training_ds.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_validation_data = './storage/data_for_pointcloud_MNIST/valid_ds.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_data = '~/Dropbox/data_for_pointcloud_MNIST/training_ds.h5'\n",
    "path_to_validation_data = '~/Dropbox/data_for_pointcloud_MNIST/valid_ds.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNC = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageClassifier(nn.Module):\n",
    "class DeepSet(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hidsize = 16\n",
    "        \n",
    "        \n",
    "        #MOD\n",
    "        self.embedding = nn.Linear(2,self.hidsize)\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        \n",
    "            # mod by feat\n",
    "            nn.Linear(self.hidsize,4096),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            # MOD\n",
    "            nn.Dropout(),\n",
    "            #num_classes = 10\n",
    "            nn.Linear(4096, NUM_CLASSES),\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "#         print('in',batch.shape)\n",
    "        # not helping\n",
    "        batch.requires_grad_(True)\n",
    "#         print(batch)\n",
    "        \n",
    "#         self.hidsize = 2048\n",
    "        \n",
    "        out = self.embedding(batch)\n",
    "        \n",
    "        out = torch.mean(out,dim=1)\n",
    "\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "#         print('out',out)\n",
    "        \n",
    "#         print ('out',out.shape)\n",
    "#         print('out.grad_fn',out.grad_fn)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset_train = CustomDataset(path_to_training_data)\n",
    "        batch_sampler = CustomBatchSampler(dataset_train.n_points, batch_size=50)\n",
    "#         self.Ns = batch_sampler.entries\n",
    "        return DataLoader(dataset_train,batch_sampler=batch_sampler,num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset_val = CustomDataset(path_to_validation_data)\n",
    "        batch_sampler_val = CustomBatchSampler(dataset_val.n_points,batch_size=50)\n",
    "        return DataLoader(dataset_val,batch_sampler=batch_sampler_val,num_workers=4)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "#         print('batch_ix',batch_idx)\n",
    "#         print('batch',batch)\n",
    "        minib,target = batch\n",
    "        yhat = self(minib)\n",
    "#         print('yhat',yhat)\n",
    "#         print('target',target)\n",
    "#         out = self.forward(minib)\n",
    "        loss = LOSS_FUNC(yhat,target)\n",
    "        loss.requires_grad_(True)\n",
    "#         print('loss.requires_grad',loss.requires_grad)\n",
    "#         print('loss',loss)\n",
    "        \n",
    "    \n",
    "        # try this\n",
    "#         pred = yhat.argmax(dim=1, keepdim=True) #get ix of max log-proba\n",
    "#         correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "#         accuracy = correct/len(minib)\n",
    "        \n",
    "#         # add logging\n",
    "#         logs = {'train_loss': loss,'train_accuracy':accuracy}\n",
    "        logs = {'train_loss':loss}\n",
    "#         print('logs',logs)\n",
    "        return {'loss': loss, 'log': logs}\n",
    "#         return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        minib, target = batch\n",
    "#         out = self.forward(minib)\n",
    "        yhat = self(minib)\n",
    "#         print('valid y_hat',yhat.shape)\n",
    "        loss = LOSS_FUNC(yhat,target)\n",
    "        \n",
    "        pred = yhat.argmax(dim=1, keepdim=True) #get ix of max log-proba\n",
    "#         print('valid pred',pred.shape)\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        #mod to torch.tensor forr avg_acc\n",
    "        accuracy = torch.tensor(correct/len(minib))\n",
    "        \n",
    "#         logs = {'train_loss': loss,'val_accuracy':accuracy}\n",
    "#         return {'val_loss': loss, 'log': logs}\n",
    "        \n",
    "        return {'val_step_loss':loss, 'val_step_acc':accuracy}\n",
    "        \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #TODO imbalanced given each batch is of a different size...\n",
    "        avg_loss = torch.stack([x['val_step_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_step_acc'] for x in outputs]).mean()\n",
    "        \n",
    "        \n",
    "        tqdm_dict = {'val_acc': avg_acc.item()}\n",
    "\n",
    "        # show val_acc in progress bar but only log val_loss\n",
    "        results = {\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': {'val_acc': avg_acc.item(),\n",
    "                    'val_loss': avg_loss.item()\n",
    "                   }\n",
    "        }\n",
    "        return results\n",
    "#         logs = {'val_loss':avg_loss}\n",
    "#         return {'avg_val_loss':avg_loss, 'log':tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "# class MyPrintingCallback(Callback):\n",
    "\n",
    "#     def on_init_start(self, trainer):\n",
    "#         print('Starting to init trainer!')\n",
    "\n",
    "#     def on_init_end(self, trainer):\n",
    "#         print('Trainer is init now')\n",
    "\n",
    "#     def on_train_end(self, trainer, pl_module):\n",
    "#         print('do something when training ends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(callbacks=[MyPrintingCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'deepsettest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "COMET INFO: old comet version (3.1.8) detected. current: 3.1.11 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ren-e1011/dl1010-a3/cad15a2b22cb423c85c3f0c6d419763b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comet_logger = CometLogger(\n",
    "    api_key='n0QCcEJ7YYeDUkff49kqLEdeJ',\n",
    "    workspace=\"ren-e1011\",\n",
    "    project_name=\"dl1010-a3\", \n",
    "    experiment_name=experiment  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(fast_dev_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type       | Params\n",
      "----------------------------------------\n",
      "0 | embedding    | Linear     | 48    \n",
      "1 | classifier   | Sequential | 16 M  \n",
      "2 | classifier.0 | Linear     | 69 K  \n",
      "3 | classifier.1 | ReLU       | 0     \n",
      "4 | classifier.2 | Dropout    | 0     \n",
      "5 | classifier.3 | Linear     | 16 M  \n",
      "6 | classifier.4 | ReLU       | 0     \n",
      "7 | classifier.5 | Dropout    | 0     \n",
      "8 | classifier.6 | Linear     | 40 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ac58dd38584b208a3c370de75321b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939cd39f3f9049a095b3f7bf36582f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renee/opt/anaconda3/envs/dl1010/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: The metric you returned 6.509487628936768 must be a Torch.Tensor instance, checkpoint not saved HINT: what is the value of val_loss in validation_end()?\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_loader,net):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    total_number = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for x,y in data_loader:\n",
    "        \n",
    "        prediction = net(x).data.numpy()\n",
    "        \n",
    "        prediction = np.argmax(prediction,axis=1)\n",
    "        \n",
    "        correct = len( np.where(prediction==y.data.numpy())[0] )\n",
    "        \n",
    "        total_correct+=correct\n",
    "        total_number+=x.shape[0]\n",
    "        \n",
    "    return total_correct/float(total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1135"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = CustomDataset(path_to_validation_data)\n",
    "batch_size = 50\n",
    "batch_sampler_test_ds = CustomBatchSampler(test_ds.n_points, batch_size)\n",
    "data_loader_test = DataLoader(test_ds, batch_sampler=batch_sampler_test_ds)\n",
    "\n",
    "compute_accuracy(data_loader_test,net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1010",
   "language": "python",
   "name": "dl1010"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
