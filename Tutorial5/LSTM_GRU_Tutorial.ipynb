{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_GRU_Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPYrxzx2nAS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTSdTair1RxB",
        "colab_type": "text"
      },
      "source": [
        "## Implementation in basic numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgAQGhSn1Ur3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def dsigmoid(y):\n",
        "    return y * (1 - y)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def dtanh(y):\n",
        "    return 1 - y * y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6PLY_ah1vXx",
        "colab_type": "text"
      },
      "source": [
        "#### hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zteazoLn1xzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_size = 100 # Size of the hidden layer\n",
        "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning rate\n",
        "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
        "X_size = 1000\n",
        "H_size, z_size = 100, 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnsQ9fG21lC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "        self.name = name\n",
        "        self.v = value #parameter value\n",
        "        self.d = np.zeros_like(value) #derivative\n",
        "        self.m = np.zeros_like(value) #momentum for AdaGrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tJiA1an1c2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', \n",
        "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i',\n",
        "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C',\n",
        "                         np.random.randn(H_size, z_size) * weight_sd)\n",
        "        self.b_C = Param('b_C',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o',\n",
        "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v',\n",
        "                         np.random.randn(X_size, H_size) * weight_sd)\n",
        "        self.b_v = Param('b_v',\n",
        "                         np.zeros((X_size, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktKDyCyJ2TWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (H_size, 1)\n",
        "    assert C_prev.shape == (H_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o28F2EOO3Cn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + H_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (H_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:H_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oU1DirnAkEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## --- in short LSTM :\n",
        "def lstm(inputs, state, params):\n",
        "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
        "     W_hq, b_q] = params\n",
        "    (H, C) = state\n",
        "    outputs = []\n",
        "    for X in inputs:\n",
        "        I = npx.sigmoid(np.dot(X, W_xi) + np.dot(H, W_hi) + b_i)\n",
        "        F = npx.sigmoid(np.dot(X, W_xf) + np.dot(H, W_hf) + b_f)\n",
        "        O = npx.sigmoid(np.dot(X, W_xo) + np.dot(H, W_ho) + b_o)\n",
        "        C_tilda = np.tanh(np.dot(X, W_xc) + np.dot(H, W_hc) + b_c)\n",
        "        C = F * C + I * C_tilda\n",
        "        H = O * np.tanh(C)\n",
        "        Y = np.dot(H, W_hq) + b_q\n",
        "        outputs.append(Y)\n",
        "    return np.concatenate(outputs, axis=0), (H, C)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmgOIAw-21v",
        "colab_type": "text"
      },
      "source": [
        "## GRU numpy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hr9p-nRAZ-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru(inputs, state, params):\n",
        "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "    H, = state\n",
        "    outputs = []\n",
        "    for X in inputs:\n",
        "        Z = npx.sigmoid(np.dot(X, W_xz) + np.dot(H, W_hz) + b_z)\n",
        "        R = npx.sigmoid(np.dot(X, W_xr) + np.dot(H, W_hr) + b_r)\n",
        "        H_tilda = np.tanh(np.dot(X, W_xh) + np.dot(R * H, W_hh) + b_h)\n",
        "        H = Z * H + (1 - Z) * H_tilda\n",
        "        Y = np.dot(H, W_hq) + b_q\n",
        "        outputs.append(Y)\n",
        "    return np.concatenate(outputs, axis=0), (H,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF3M3S_ipvoi",
        "colab_type": "text"
      },
      "source": [
        "### Basic structure of a RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4CPigCipzKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNCellBase(nn.Module):\n",
        "    __constants__ = ['input_size', 'hidden_size', 'bias']\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias, num_chunks):\n",
        "        super(RNNCellBase, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.weight_ih = Parameter(torch.Tensor(num_chunks * hidden_size, input_size))\n",
        "        self.weight_hh = Parameter(torch.Tensor(num_chunks * hidden_size, hidden_size))\n",
        "        if bias:\n",
        "            self.bias_ih = Parameter(torch.Tensor(num_chunks * hidden_size))\n",
        "            self.bias_hh = Parameter(torch.Tensor(num_chunks * hidden_size))\n",
        "        else:\n",
        "            self.register_parameter('bias_ih', None)\n",
        "            self.register_parameter('bias_hh', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = '{input_size}, {hidden_size}'\n",
        "        if 'bias' in self.__dict__ and self.bias is not True:\n",
        "            s += ', bias={bias}'\n",
        "        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n",
        "            s += ', nonlinearity={nonlinearity}'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def check_forward_input(self, input):\n",
        "        if input.size(1) != self.input_size:\n",
        "            raise RuntimeError(\n",
        "                \"input has inconsistent input_size: got {}, expected {}\".format(\n",
        "                    input.size(1), self.input_size))\n",
        "\n",
        "    def check_forward_hidden(self, input, hx, hidden_label=''):\n",
        "        # type: (Tensor, Tensor, str) -> None\n",
        "        if input.size(0) != hx.size(0):\n",
        "            raise RuntimeError(\n",
        "                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n",
        "                    input.size(0), hidden_label, hx.size(0)))\n",
        "\n",
        "        if hx.size(1) != self.hidden_size:\n",
        "            raise RuntimeError(\n",
        "                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n",
        "                    hidden_label, hx.size(1), self.hidden_size))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            init.uniform_(weight, -stdv, stdv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKRD8I_LrwO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNCell(RNNCellBase):\n",
        "\n",
        "    __constants__ = ['input_size', 'hidden_size', 'bias', 'nonlinearity']\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
        "        super(RNNCell, self).__init__(input_size, hidden_size, bias, num_chunks=1)\n",
        "        self.nonlinearity = nonlinearity\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        # type: (Tensor, Optional[Tensor]) -> Tensor\n",
        "        self.check_forward_input(input)\n",
        "        if hx is None:\n",
        "            hx = torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)\n",
        "        self.check_forward_hidden(input, hx, '')\n",
        "        if self.nonlinearity == \"tanh\":\n",
        "            ret = _VF.rnn_tanh_cell(\n",
        "                input, hx,\n",
        "                self.weight_ih, self.weight_hh,\n",
        "                self.bias_ih, self.bias_hh,\n",
        "            )\n",
        "        elif self.nonlinearity == \"relu\":\n",
        "            ret = _VF.rnn_relu_cell(\n",
        "                input, hx,\n",
        "                self.weight_ih, self.weight_hh,\n",
        "                self.bias_ih, self.bias_hh,\n",
        "            )\n",
        "        else:\n",
        "            ret = input  # TODO: remove when jit supports exception flow\n",
        "            raise RuntimeError(\n",
        "                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n",
        "        return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT1nl3659dwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMCell(RNNCellBase):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(LSTMCell, self).__init__(input_size, hidden_size, bias, num_chunks=4)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        # type: (Tensor, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]\n",
        "        self.check_forward_input(input)\n",
        "        if hx is None:\n",
        "            zeros = torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)\n",
        "            hx = (zeros, zeros)\n",
        "        self.check_forward_hidden(input, hx[0], '[0]')\n",
        "        self.check_forward_hidden(input, hx[1], '[1]')\n",
        "        return _VF.lstm_cell(\n",
        "            input, hx,\n",
        "            self.weight_ih, self.weight_hh,\n",
        "            self.bias_ih, self.bias_hh,\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S4nQp2q9pHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUCell(RNNCellBase):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__(input_size, hidden_size, bias, num_chunks=3)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        # type: (Tensor, Optional[Tensor]) -> Tensor\n",
        "        self.check_forward_input(input)\n",
        "        if hx is None:\n",
        "            hx = torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)\n",
        "        self.check_forward_hidden(input, hx, '')\n",
        "        return _VF.gru_cell(\n",
        "            input, hx,\n",
        "            self.weight_ih, self.weight_hh,\n",
        "            self.bias_ih, self.bias_hh,\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNC5eXLxnTP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = nn.RNNCell(10, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K45pV41opAOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.RNNCell??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNpjvFTXpTeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch._VF.lstm??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qftVcGbt6h8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}